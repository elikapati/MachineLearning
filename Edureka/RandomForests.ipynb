{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'library' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-5707f1a2b3d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlibrary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mggplot2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mlibrary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcowplot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mlibrary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandomForest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m## NOTE: The data used in this demo comes from the UCI machine learning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'library' is not defined"
     ]
    }
   ],
   "source": [
    "library(ggplot2)\n",
    "library(cowplot)\n",
    "library(randomForest)\n",
    " \n",
    "## NOTE: The data used in this demo comes from the UCI machine learning\n",
    "## repository.\n",
    "## http://archive.ics.uci.edu/ml/index.php\n",
    "## Specifically, this is the heart disease data set.\n",
    "## http://archive.ics.uci.edu/ml/datasets/Heart+Disease\n",
    " \n",
    "url <- \"http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\"\n",
    " \n",
    "data <- read.csv(url, header=FALSE)\n",
    " \n",
    "#####################################\n",
    "##\n",
    "## Reformat the data so that it is\n",
    "## 1) Easy to use (add nice column names)\n",
    "## 2) Interpreted correctly by randomForest..\n",
    "##\n",
    "#####################################\n",
    "head(data) # you see data, but no column names\n",
    " \n",
    "colnames(data) <- c(\n",
    "  \"age\",\n",
    "  \"sex\",# 0 = female, 1 = male\n",
    "  \"cp\", # chest pain\n",
    "          # 1 = typical angina,\n",
    "          # 2 = atypical angina,\n",
    "          # 3 = non-anginal pain,\n",
    "          # 4 = asymptomatic\n",
    "  \"trestbps\", # resting blood pressure (in mm Hg)\n",
    "  \"chol\", # serum cholestoral in mg/dl\n",
    "  \"fbs\",  # fasting blood sugar greater than 120 mg/dl, 1 = TRUE, 0 = FALSE\n",
    "  \"restecg\", # resting electrocardiographic results\n",
    "          # 1 = normal\n",
    "          # 2 = having ST-T wave abnormality\n",
    "          # 3 = showing probable or definite left ventricular hypertrophy\n",
    "  \"thalach\", # maximum heart rate achieved\n",
    "  \"exang\",   # exercise induced angina, 1 = yes, 0 = no\n",
    "  \"oldpeak\", # ST depression induced by exercise relative to rest\n",
    "  \"slope\", # the slope of the peak exercise ST segment\n",
    "          # 1 = upsloping\n",
    "          # 2 = flat\n",
    "          # 3 = downsloping\n",
    "  \"ca\", # number of major vessels (0-3) colored by fluoroscopy\n",
    "  \"thal\", # this is short of thalium heart scan\n",
    "          # 3 = normal (no cold spots)\n",
    "          # 6 = fixed defect (cold spots during rest and exercise)\n",
    "          # 7 = reversible defect (when cold spots only appear during exercise)\n",
    "  \"hd\" # (the predicted attribute) - diagnosis of heart disease\n",
    "          # 0 if less than or equal to 50% diameter narrowing\n",
    "          # 1 if greater than 50% diameter narrowing\n",
    "  )\n",
    " \n",
    "head(data) # now we have data and column names\n",
    " \n",
    "str(data) # this shows that we need to tell R which columns contain factors\n",
    "          # it also shows us that there are some missing values. There are \"?\"s\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-40f702939de3>, line 67)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-40f702939de3>\"\u001b[1;36m, line \u001b[1;32m67\u001b[0m\n\u001b[1;33m    data[data$sex == 0,]$sex <- \"F\"\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "         # in the dataset.\n",
    " \n",
    "## First, replace \"?\"s with NAs.\n",
    "data[data == \"?\"] <- NA\n",
    " \n",
    "## Now add factors for variables that are factors and clean up the factors\n",
    "## that had missing data...\n",
    "data[data$sex == 0,]$sex <- \"F\"\n",
    "data[data$sex == 1,]$sex <- \"M\"\n",
    "data$sex <- as.factor(data$sex)\n",
    " \n",
    "data$cp <- as.factor(data$cp)\n",
    "data$fbs <- as.factor(data$fbs)\n",
    "data$restecg <- as.factor(data$restecg)\n",
    "data$exang <- as.factor(data$exang)\n",
    "data$slope <- as.factor(data$slope)\n",
    " \n",
    "data$ca <- as.integer(data$ca) # since this column had \"?\"s in it (which\n",
    "                               # we have since converted to NAs) R thinks that\n",
    "                               # the levels for the factor are strings, but\n",
    "                               # we know they are integers, so we'll first\n",
    "                               # convert the strings to integiers...\n",
    "data$ca <- as.factor(data$ca)  # ...then convert the integers to factor levels\n",
    " \n",
    "data$thal <- as.integer(data$thal) # \"thal\" also had \"?\"s in it.\n",
    "data$thal <- as.factor(data$thal)\n",
    " \n",
    "## This next line replaces 0 and 1 with \"Healthy\" and \"Unhealthy\"\n",
    "data$hd <- ifelse(test=data$hd == 0, yes=\"Healthy\", no=\"Unhealthy\")\n",
    "data$hd <- as.factor(data$hd) # Now convert to a factor\n",
    " \n",
    "str(data) ## this shows that the correct columns are factors and we've replaced\n",
    "  ## \"?\"s with NAs because \"?\" no longer appears in the list of factors\n",
    "  ## for \"ca\" and \"thal\"\n",
    " \n",
    "#####################################\n",
    "##\n",
    "## Now we are ready to build a random forest.\n",
    "##\n",
    "#####################################\n",
    "set.seed(42)\n",
    " \n",
    "## NOTE: For most machine learning methods, you need to divide the data\n",
    "## manually into a \"training\" set and a \"test\" set. This allows you to train\n",
    "## the method using the training data, and then test it on data it was not\n",
    "## originally trained on.\n",
    "##\n",
    "## In contrast, Random Forests split the data into \"training\" and \"test\" sets\n",
    "## for you. This is because Random Forests use bootstrapped\n",
    "## data, and thus, not every sample is used to build every tree. The\n",
    "## \"training\" dataset is the bootstrapped data and the \"test\" dataset is\n",
    "## the remaining samples. The remaining samples are called\n",
    "## the \"Out-Of-Bag\" (OOB) data.\n",
    " \n",
    "## impute any missing values in the training set using proximities\n",
    "data.imputed <- rfImpute(hd ~ ., data = data, iter=6)\n",
    "## NOTE: iter = the number of iterations to run. Breiman says 4 to 6 iterations\n",
    "## is usually good enough. With this dataset, when we set iter=6, OOB-error\n",
    "## bounces around between 17% and 18%. When we set iter=20,\n",
    "# set.seed(42)\n",
    "# data.imputed <- rfImpute(hd ~ ., data = data, iter=20)\n",
    "## we get values a little better and a little worse, so doing more\n",
    "## iterations doesn't improve the situation.\n",
    "##\n",
    "## NOTE: If you really want to micromanage how rfImpute(),\n",
    "## you can change the number of trees it makes (the default is 300) and the\n",
    "## number of variables that it will consider at each step.\n",
    " \n",
    "## Now we are ready to build a random forest.\n",
    " \n",
    "## NOTE: If the thing we're trying to predict (in this case it is\n",
    "## whether or not someone has heart disease) is a continuous number\n",
    "## (i.e. \"weight\" or \"height\"), then by default, randomForest() will set\n",
    "## \"mtry\", the number of variables to consider at each step,\n",
    "## to the total number of variables divided by 3 (rounded down), or to 1\n",
    "## (if the division results in a value less than 1).\n",
    "## If the thing we're trying to predict is a \"factor\" (i.e. either \"yes/no\"\n",
    "## or \"ranked\"), then randomForest() will set mtry to\n",
    "## the square root of the number of variables (rounded down to the next\n",
    "## integer value).\n",
    " \n",
    "## In this example, \"hd\", the thing we are trying to predict, is a factor and\n",
    "## there are 13 variables. So by default, randomForest() will set\n",
    "## mtry = sqrt(13) = 3.6 rounded down = 3\n",
    "## Also, by default random forest generates 500 trees (NOTE: rfImpute() only\n",
    "## generates 300 tress by default)\n",
    "model <- randomForest(hd ~ ., data=data.imputed, proximity=TRUE)\n",
    " \n",
    "## RandomForest returns all kinds of things\n",
    "model # gives us an overview of the call, along with...\n",
    "      # 1) The OOB error rate for the forest with ntree trees.\n",
    "      #    In this case ntree=500 by default\n",
    "      # 2) The confusion matrix for the forest with ntree trees.\n",
    "      #    The confusion matrix is laid out like this:\n",
    "#\n",
    "#                Healthy                      Unhealthy\n",
    "#          --------------------------------------------------------------\n",
    "# Healthy  | Number of healthy people   | Number of healthy people      |\n",
    "#          | correctly called \"healthy\" | incorectly called \"unhealthy\" |\n",
    "#          | by the forest.             | by the forest                 |\n",
    "#          --------------------------------------------------------------\n",
    "# Unhealthy| Number of unhealthy people | Number of unhealthy peole     |\n",
    "#          | incorrectly called         | correctly called \"unhealthy\"  |\n",
    "#          | \"healthy\" by the forest    | by the forest                 |\n",
    "#          --------------------------------------------------------------\n",
    " \n",
    "## Now check to see if the random forest is actually big enough...\n",
    "## Up to a point, the more trees in the forest, the better. You can tell when\n",
    "## you've made enough when the OOB no longer improves.\n",
    "oob.error.data <- data.frame(\n",
    "  Trees=rep(1:nrow(model$err.rate), times=3),\n",
    "  Type=rep(c(\"OOB\", \"Healthy\", \"Unhealthy\"), each=nrow(model$err.rate)),\n",
    "  Error=c(model$err.rate[,\"OOB\"],\n",
    "    model$err.rate[,\"Healthy\"],\n",
    "    model$err.rate[,\"Unhealthy\"]))\n",
    " \n",
    "ggplot(data=oob.error.data, aes(x=Trees, y=Error)) +\n",
    "  geom_line(aes(color=Type))\n",
    "# ggsave(\"oob_error_rate_500_trees.pdf\")\n",
    " \n",
    "## Blue line = The error rate specifically for calling \"Unheathly\" patients that\n",
    "## are OOB.\n",
    "##\n",
    "## Green line = The overall OOB error rate.\n",
    "##\n",
    "## Red line = The error rate specifically for calling \"Healthy\" patients\n",
    "## that are OOB.\n",
    " \n",
    "## NOTE: After building a random forest with 500 tress, the graph does not make\n",
    "## it clear that the OOB-error has settled on a value or, if we added more\n",
    "## trees, it would continue to decrease.\n",
    "## So we do the whole thing again, but this time add more trees.\n",
    " \n",
    "model <- randomForest(hd ~ ., data=data.imputed, ntree=1000, proximity=TRUE)\n",
    "model\n",
    " \n",
    "oob.error.data <- data.frame(\n",
    "  Trees=rep(1:nrow(model$err.rate), times=3),\n",
    "  Type=rep(c(\"OOB\", \"Healthy\", \"Unhealthy\"), each=nrow(model$err.rate)),\n",
    "  Error=c(model$err.rate[,\"OOB\"],\n",
    "    model$err.rate[,\"Healthy\"],\n",
    "    model$err.rate[,\"Unhealthy\"]))\n",
    " \n",
    "ggplot(data=oob.error.data, aes(x=Trees, y=Error)) +\n",
    "  geom_line(aes(color=Type))\n",
    "# ggsave(\"oob_error_rate_1000_trees.pdf\")\n",
    " \n",
    "## After building a random forest with 1,000 trees, we get the same OOB-error\n",
    "## 16.5% and we can see convergence in the graph. So we could have gotten\n",
    "## away with only 500 trees, but we wouldn't have been sure that number\n",
    "## was enough.\n",
    " \n",
    "## If we want to compare this random forest to others with different values for\n",
    "## mtry (to control how many variables are considered at each step)...\n",
    "oob.values <- vector(length=10)\n",
    "for(i in 1:10) {\n",
    "  temp.model <- randomForest(hd ~ ., data=data.imputed, mtry=i, ntree=1000)\n",
    "  oob.values[i] <- temp.model$err.rate[nrow(temp.model$err.rate),1]\n",
    "}\n",
    "oob.values\n",
    "## [1] 0.1716172 0.1716172 0.1617162 0.1848185 0.1749175 0.1947195 0.1815182\n",
    "## [8] 0.2013201 0.1881188 0.1947195\n",
    "## The lowest value is when mtry=3, so the default setting was the best.\n",
    " \n",
    "## Now let's create an MDS-plot to show how the samples are related to each\n",
    "## other.\n",
    "##\n",
    "## Start by converting the proximity matrix into a distance matrix.\n",
    "distance.matrix <- dist(1-model$proximity)\n",
    " \n",
    "mds.stuff <- cmdscale(distance.matrix, eig=TRUE, x.ret=TRUE)\n",
    " \n",
    "## calculate the percentage of variation that each MDS axis accounts for...\n",
    "mds.var.per <- round(mds.stuff$eig/sum(mds.stuff$eig)*100, 1)\n",
    " \n",
    "## now make a fancy looking plot that shows the MDS axes and the variation:\n",
    "mds.values <- mds.stuff$points\n",
    "mds.data <- data.frame(Sample=rownames(mds.values),\n",
    "  X=mds.values[,1],\n",
    "  Y=mds.values[,2],\n",
    "  Status=data.imputed$hd)\n",
    " \n",
    "ggplot(data=mds.data, aes(x=X, y=Y, label=Sample)) +\n",
    "  geom_text(aes(color=Status)) +\n",
    "  theme_bw() +\n",
    "  xlab(paste(\"MDS1 - \", mds.var.per[1], \"%\", sep=\"\")) +\n",
    "  ylab(paste(\"MDS2 - \", mds.var.per[2], \"%\", sep=\"\")) +\n",
    "  ggtitle(\"MDS plot using (1 - Random Forest Proximities)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
