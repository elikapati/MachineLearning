{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Understand Data\n",
    "- Structure the unstructured data\n",
    "- Identify level of data (Nominal, Ordinal, Interval, Ratio)\n",
    "- Convert the features at the level to usable numeric form\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature Improvement\n",
    "- Separate dataset into features, response sets (vertical split)<br/>\n",
    "_Steps a & b can be combined in pipelines_<br/>\n",
    "- a) Identify missing values and Fill them using Imputer \n",
    "     -- Create custom Imputers for each \n",
    "        \"column type\"(quality type, quantity type) and \n",
    "        \"strategy\" then \n",
    "        invoke them in pipeline\n",
    "        (Ex, missing city with most frequent of city, missing numerical value with mean/median, etc.)\n",
    "- b) Enhance data\n",
    "     - Standardize or z-score normalize\n",
    "     - min-max normalize\n",
    "     - Row normalize (L1, L2)\n",
    "- Split data into train, test, validation sets (horizontal split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature Construction\n",
    " - Encode Categorical variables in \"Nominal\" feature\n",
    "   Ex, Transform Nominal feature 'City' with values like 'London', 'Tokyo', 'Toronto' into \n",
    "        new features viz., City_London, City_Tokyo, City_Toronto\n",
    "   Create dummyvariables with Pandas or create a custom transformer\n",
    "\n",
    " - Encode Categorical variables in \"Ordinal\" feature\n",
    "   Ex, Encode Ordinal feature 'TShirt' with values like 'x-small', 'small', 'medium', 'large', 'x-large' to\n",
    "       new values viz., 0, 1, 2, 3, 4 respectively\n",
    "   Map the ordinal values to numbers either manually or create a custom transformer (that label encodes)\n",
    "\n",
    " - Bucket Continuous features to categories\n",
    "   Ex, Transform numerical continuous feature 'Age' having values ranging from say 0 through 90 can be bucketed to\n",
    "       new categorical feature variables viz., kid, youngster, adolescent, aged\n",
    "    \n",
    " - Extend Numerical features\n",
    "   Ex, Construct new features like x.x, x^2, y^2, z^2, x.y, y.z, z.x when basic 'accelerometer reading' features, x, y, z \n",
    "       (numeric floating point values) are not sufficent to explain the hidden patterns to identify posture of a person \n",
    "       like 'working', 'standing', 'upstairs', 'downstairs', 'walking', 'talking', etc. \n",
    "\n",
    " - Construct Text specific features\n",
    "   Ex, Construct feature vectors from lengthy text employing \"Bag of words\" using CountVectorizer, TfidfVectorizer methods\n",
    "   An aggregate of text content or documents is called \"Corpus\". \n",
    "   A bag of words is achieved with the steps:\n",
    "        - Tokenizing (Converting text into tokens using delimiters)\n",
    "        - Counting (Count tokens)\n",
    "        - Normalizing (tokens are weighted with diminishing importance when they occur in the majority of documents)\n",
    "   Stemming is a common natural language processing method that allows us to stem our vocabulary, or make it smaller \n",
    "   by converting words to their roots.\n",
    "   Ex, interesting -> interest    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature Selection\n",
    "   Select best subset of features from given features using \n",
    "    - statistical based techniques\n",
    "      They rely heavily on statistical tests that are separate from our machine learning models in order to select \n",
    "      features during the training phase of our pipeline.\n",
    "      Techniques used(univariate):\n",
    "            - Pearson correlation, \n",
    "            - Hypothesis testing\n",
    "        \n",
    "    - model based techniques\n",
    "      They rely on a preprocessing step that involves training a secondary machine learning model and using that \n",
    "      model's predictive power to select features.\n",
    "\n",
    "   Both the feature selection techniques attempt to reduce the size of our data by subsetting from original features \n",
    "   only the best ones with the highest predictive power.\n",
    "    \n",
    "   Basic metrics used for the predictive performance:\n",
    "                           Basic Metrics\n",
    "                           -------------\n",
    "         Classification    Accuracy\n",
    "         Regression        RMSE\n",
    "\n",
    "   Other metrics used for the predictive performance:\n",
    "                           Other Performance Metrics\n",
    "                           -------------------------\n",
    "         Classification    True Positive rate, False Postive rate\n",
    "                           Sensitivity (True Positive rate) and Specificity\n",
    "                           False Positive rate and False Negative rate\n",
    "    \n",
    "         Regression        Mean Absolute Error\n",
    "                           R-Square\n",
    "\n",
    "                           Meta metrics\n",
    "                           ------------\n",
    "                           Time to fit/train model\n",
    "                           Time to predict\n",
    "                           Data size to persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
